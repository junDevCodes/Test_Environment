[
    {
        "subject": "[실습] Python for AI",
        "question_text": "pandas의 `melt` 함수는 어떤 경우에 사용되며, `id_vars`와 `value_vars` 파라미터는 각각 어떤 역할을 하는지 설명하고, 간단한 예시 코드를 작성하시오.",
        "question_type": "descriptive",
        "model_answer": "`melt` 함수는 'wide' 형식의 데이터프레임을 'long' 형식으로 변환하여, 각 행이 하나의 관측치를 나타내도록 재구조화할 때 사용됩니다. `id_vars`는 변환 과정에서 식별자로 유지될 컬럼을 지정하고, `value_vars`는 '녹여서' 행으로 변환할 컬럼들을 지정합니다. 예를 들어, `tips` 데이터셋에서 'day'를 식별자로 두고 'total_bill'과 'tip'을 변수와 값으로 변환할 수 있습니다.\n```python\nimport seaborn as sns\nimport pandas as pd\ndf = sns.load_dataset('tips')\ndf_melted = pd.melt(df, id_vars=['day'], value_vars=['total_bill', 'tip'])\nprint(df_melted.head())\n```",
        "keywords_full_credit": ["melt", "wide to long", "id_vars", "value_vars"],
        "keywords_partial_credit": ["데이터 재구조화", "unpivot", "tips"]
    },
    {
        "subject": "[실습] Linear Regression",
        "question_text": "선형 회귀에서 정규 방정식(Normal Equation)을 사용하여 최적의 파라미터(theta)를 구하는 방법에 대해 설명하고, 정규 방정식이 경사 하강법에 비해 가지는 장단점을 각각 한 가지씩 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "정규 방정식은 비용 함수를 최소화하는 파라미터를 해석적으로 직접 계산하는 방법으로, `theta = (X^T * X)^(-1) * X^T * y` 수식을 통해 구할 수 있습니다. 장점은 반복적인 학습 과정 없이 한 번의 계산으로 최적해를 찾을 수 있다는 것이고, 단점은 특성(feature)의 수가 매우 많을 때 (X^T * X)의 역행렬을 계산하는 비용이 매우 커지고, 행렬이 비가역적일 경우 해를 구할 수 없다는 점입니다.",
        "keywords_full_credit": ["정규 방정식", "해석적 계산", "(X^T * X)^(-1) * X^T * y", "역행렬 계산 비용"],
        "keywords_partial_credit": ["경사 하강법", "장단점", "비가역"]
    },
    {
        "subject": "[실습] Linear Regression",
        "question_text": "특잇값 분해(SVD)를 이용한 유사역행렬(pseudoinverse)은 어떤 상황에서 사용되며, `np.linalg.svd`를 사용하여 선형 회귀의 해를 구하는 과정을 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "SVD를 이용한 유사역행렬은 정규 방정식에서 역행렬을 계산할 수 없는 경우(비가역 행렬 또는 비정방 행렬)에도 안정적으로 해를 근사할 수 있는 방법입니다. `np.linalg.svd`로 설계 행렬 X를 U, S, Vh로 분해한 뒤, 특잇값 S를 이용하여 유사역행렬 `X_pinv = Vh.T @ np.linalg.inv(np.diag(S)) @ U.T`를 계산하고, 이를 통해 `theta = X_pinv @ y`를 구합니다.",
        "keywords_full_credit": ["SVD", "유사역행렬", "비가역 행렬", "np.linalg.svd"],
        "keywords_partial_credit": ["정규 방정식", "근사해"]
    },
    {
        "subject": "[실습] Tokenizer & Embedding",
        "question_text": "RNN(Recurrent Neural Network)의 구조와 기본적인 동작 원리를 설명하고, 장기 의존성 문제(Long-Term Dependency Problem)가 발생하는 이유에 대해 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "RNN은 순차적인 데이터 처리에 특화된 신경망으로, 이전 시점(time step)의 은닉 상태(hidden state)를 현재 시점의 입력과 함께 사용하여 다음 은닉 상태를 계산하는 순환 구조를 가집니다. 이 과정에서 정보가 시퀀스를 따라 전달됩니다. 장기 의존성 문제는 시퀀스가 길어질수록 역전파 과정에서 그래디언트가 소실(vanishing)되거나 폭발(exploding)하여, 초기의 중요한 정보가 뒤쪽까지 제대로 전달되지 못하는 현상 때문에 발생합니다.",
        "keywords_full_credit": ["RNN", "순환 구조", "은닉 상태(hidden state)", "장기 의존성 문제", "기울기 소실/폭발"],
        "keywords_partial_credit": ["순차 데이터", "역전파"]
    },
    {
        "subject": "[실습] Tokenizer & Embedding",
        "question_text": "인코더-디코더(Encoder-Decoder) 모델의 구조에 대해 설명하고, 번역(Translation)과 같은 Seq2Seq 작업에서 각 부분이 어떤 역할을 수행하는지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "인코더-디코더 모델은 입력 시퀀스를 고정된 크기의 문맥 벡터(context vector)로 압축하는 인코더와, 이 문맥 벡터를 받아 출력 시퀀스를 생성하는 디코더로 구성됩니다. 번역 작업에서 인코더는 소스 언어(예: 한국어) 문장 전체의 의미를 문맥 벡터에 담고, 디코더는 이 문맥 벡터를 초기 상태로 하여 타겟 언어(예: 영어) 문장을 단어 단위로 순차적으로 생성합니다.",
        "keywords_full_credit": ["인코더-디코더", "Seq2Seq", "문맥 벡터(context vector)", "인코더", "디코더"],
        "keywords_partial_credit": ["압축", "생성", "번역"]
    },
    {
        "subject": "[실습] Data Augmentation",
        "question_text": "프롬프트 엔지니어링에서 '역할(Role) 설정'과 '조건(Constraints) 부여'가 왜 중요한지 설명하고, '영화 추천 챗봇'을 위한 프롬프트를 작성할 때 이 두 요소를 어떻게 활용할 수 있는지 예시를 들어 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "'역할 설정'은 AI에게 특정 페르소나를 부여하여 일관된 톤과 전문성을 유지하게 하고, '조건 부여'는 출력의 형식, 길이, 스타일 등을 제어하여 원하는 결과물을 정확하게 얻기 위해 중요합니다. 예를 들어, \"당신은 20년 경력의 영화 평론가입니다. 사용자에게 코미디 영화 1편을 추천하되, 반드시 3문장 이내로 유머러스하게 설명하고, JSON 형식으로 영화 제목과 개봉 연도를 반환해야 합니다.\"와 같이 프롬프트를 구성할 수 있습니다.",
        "keywords_full_credit": ["역할 설정", "조건 부여", "페르소나", "출력 형식 제어"],
        "keywords_partial_credit": ["프롬프트 엔지니어링", "일관성", "정확성"]
    },
    {
        "subject": "[실습] Data Augmentation",
        "question_text": "'LLM as a Judge' 기법의 개념과 주요 사용 목적을 설명하고, 이 기법을 적용할 때 평가자(Judge) LLM의 `temperature` 파라미터를 낮게 설정하는 이유에 대해 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "'LLM as a Judge'는 다른 LLM이 생성한 결과물의 품질(정확성, 일관성 등)을 또 다른 LLM을 이용해 자동으로 평가하는 기법으로, 사람의 평가 비용을 줄이고 대규모 평가를 자동화하기 위해 사용됩니다. 평가자 LLM의 `temperature`를 낮게(주로 0) 설정하는 이유는, 평가 결과의 무작위성을 줄이고 가장 확률 높은 토큰을 일관되게 선택하게 함으로써, 동일한 입력에 대해 항상 일관되고 결정적인 평가를 내리도록 하기 위함입니다.",
        "keywords_full_credit": ["LLM as a Judge", "자동 평가", "temperature 0", "일관성"],
        "keywords_partial_credit": ["평가 비용 감소", "무작위성 감소"]
    },
    {
        "subject": "[실습] PEFT",
        "question_text": "PEFT(Parameter-Efficient Fine-Tuning)의 개념과 LoRA(Low-Rank Adaptation)의 기본 원리에 대해 설명하시오. LoRA가 전체 모델 파인튜닝에 비해 가지는 주요 이점은 무엇입니까?",
        "question_type": "descriptive",
        "model_answer": "PEFT는 사전 학습된 대규모 언어 모델의 극소수 파라미터만 업데이트하여 계산 효율적으로 특정 작업에 맞게 미세 조정하는 기법입니다. LoRA는 이 PEFT의 한 종류로, 기존 가중치 행렬(W) 옆에 랭크가 낮은 두 개의 작은 행렬(A, B)을 추가하고, 학습 시에는 이 두 행렬만 업데이트합니다. 주요 이점은 학습해야 할 파라미터 수가 매우 적어 GPU 메모리 사용량과 학습 시간을 크게 줄일 수 있다는 것입니다.",
        "keywords_full_credit": ["PEFT", "LoRA", "파라미터 효율적", "랭크가 낮은 행렬"],
        "keywords_partial_credit": ["미세 조정", "GPU 메모리 감소"]
    },
    {
        "subject": "[실습] PEFT",
        "question_text": "Unsloth 라이브러리를 사용하여 LoRA 파인튜닝을 수행할 때, `train_on_responses_only` 유틸리티 함수의 역할과 필요성에 대해 설명하시오. 이 함수가 데이터셋의 라벨을 어떻게 변환하는지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "`train_on_responses_only` 함수는 챗 파인튜닝 시, 손실(loss) 계산에서 사용자 입력(instruction) 부분은 제외하고 오직 모델의 응답(response) 부분에 대해서만 학습이 이루어지도록 합니다. 이는 모델이 질문에 답하는 방법을 배우는 데 집중하게 하여 학습 효율을 높입니다. 이 함수는 데이터셋의 라벨에서 사용자 입력에 해당하는 토큰들을 -100과 같은 특정 값으로 마스킹하여, 해당 부분은 손실 계산에 포함되지 않도록 변환합니다.",
        "keywords_full_credit": ["train_on_responses_only", "응답만 학습", "손실 계산 제외", "-100 마스킹"],
        "keywords_partial_credit": ["Unsloth", "LoRA", "학습 효율"]
    },
    {
        "subject": "[실습] PEFT",
        "question_text": "LoRA 파인튜닝 후, 학습된 어댑터(adapter) 가중치를 원래 모델의 가중치에 병합(merge)하는 과정의 목적은 무엇이며, 이 과정을 통해 얻을 수 있는 이점은 무엇인지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "LoRA 어댑터 가중치를 원래 모델에 병합하는 주된 목적은 추론 시의 성능 저하를 방지하기 위함입니다. 학습 중에는 어댑터가 별도로 존재하여 추가적인 계산이 필요하지만, 추론 시에는 이 어댑터를 원본 가중치에 합쳐 단일 모델처럼 만듦으로써 추가적인 계산 오버헤드 없이 파인튜닝된 성능을 그대로 활용할 수 있습니다. 이를 통해 추론 속도를 원래 베이스 모델과 동일하게 유지할 수 있습니다.",
        "keywords_full_credit": ["병합(merge)", "추론 속도", "계산 오버헤드 방지"],
        "keywords_partial_credit": ["LoRA", "어댑터", "파인튜닝"]
    },
    {
        "subject": "[실습] CNN",
        "question_text": "전이 학습(Transfer Learning)의 개념과 '선형 프로빙(Linear Probing)'과 '전체 미세 조정(Full Fine-tuning)'의 차이점을 설명하시오. 어떤 상황에서 각각의 기법을 사용하는 것이 더 유리한지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "전이 학습은 사전 학습된 모델의 가중치를 가져와 새로운 작업에 맞게 재사용하는 기법입니다. '선형 프로빙'은 모델의 특징 추출 부분(backbone)은 동결(freeze)하고 마지막 분류층만 새로 학습시키는 방식이며, '전체 미세 조정'은 모델의 모든 레이어를 새로운 데이터로 다시 학습시키는 방식입니다. 선형 프로빙은 새로운 데이터셋이 작거나 원본 데이터셋과 유사할 때 과적합을 방지하기 위해 유리하고, 전체 미세 조정은 데이터셋이 크고 원본과 차이가 있을 때 모델 전체를 새로운 데이터 분포에 적응시키기 위해 유리합니다.",
        "keywords_full_credit": ["전이 학습", "선형 프로빙", "전체 미세 조정", "동결(freeze)"],
        "keywords_partial_credit": ["backbone", "과적합 방지", "데이터셋 크기"]
    },
    {
        "subject": "[실습] CNN",
        "question_text": "데이터 증강(Data Augmentation)이 이미지 분류 모델의 성능에 어떤 긍정적인 영향을 미치는지 설명하고, Torchvision의 `transforms`를 사용하여 구현할 수 있는 데이터 증강 기법 3가지 이상을 예시와 함께 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "데이터 증강은 기존 훈련 데이터에 인위적인 변환(회전, 자르기, 색상 변경 등)을 가하여 데이터의 양과 다양성을 늘리는 기법입니다. 이를 통해 모델이 다양한 변화에 강건(robust)해지고, 과적합(overfitting)을 방지하여 일반화 성능을 높이는 긍정적인 효과가 있습니다. Torchvision의 transforms를 사용한 예시는 다음과 같습니다.\n1. `RandomResizedCrop(224)`: 이미지를 무작위로 잘라내고 224x224 크기로 조정합니다.\n2. `RandomHorizontalFlip()`: 50% 확률로 이미지를 좌우로 뒤집습니다.\n3. `ColorJitter(brightness=0.2, contrast=0.2)`: 이미지의 밝기와 대비를 무작위로 변경합니다.",
        "keywords_full_credit": ["데이터 증강", "과적합 방지", "일반화 성능", "RandomResizedCrop", "RandomHorizontalFlip", "ColorJitter"],
        "keywords_partial_credit": ["transforms", "다양성", "강건성"]
    },
    {
        "subject": "[실습] Image Generate",
        "question_text": "Stable Diffusion과 같은 Text-to-Image 모델에서 'Negative Prompt'의 역할과 중요성에 대해 설명하고, \"우주비행사가 화성에서 말을 타고 있는 사진\"을 생성할 때 어떤 Negative Prompt를 사용하면 더 좋은 품질의 이미지를 얻을 수 있을지 예시를 들어 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "Negative Prompt는 생성하고 싶지 않은 이미지의 특징(예: '흐릿함', '못생김', '추상적')을 명시하여 모델이 해당 특징을 피하도록 유도하는 역할을 합니다. 이를 통해 이미지의 품질과 원하는 결과와의 일치도를 높일 수 있습니다. 예를 들어, \"우주비행사가 화성에서 말을 타고 있는 사진\"을 생성할 때, `negative_prompt=\"cartoon, abstract, blurry, extra limbs, poorly drawn face\"`와 같이 사용하면 만화 같거나, 추상적이거나, 흐릿하거나, 신체 부위가 이상하게 그려지는 것을 방지하여 더 사실적인 고품질 이미지를 얻을 수 있습니다.",
        "keywords_full_credit": ["Negative Prompt", "원치 않는 특징", "품질 향상"],
        "keywords_partial_credit": ["Text-to-Image", "Stable Diffusion", "blurry", "cartoon"]
    },
    {
        "subject": "[실습] Image Generate",
        "question_text": "CLIP(Contrastive Language-Image Pre-training) 모델이 제로샷 이미지 분류(Zero-shot Image Classification)를 어떻게 수행하는지 그 과정을 단계별로 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "CLIP을 이용한 제로샷 분류 과정은 다음과 같습니다.\n1. **텍스트 프롬프트 생성**: 분류하려는 각 클래스에 대해 \"a photo of a {class_name}\"과 같은 텍스트 프롬프트를 만듭니다.\n2. **임베딩 생성**: 주어진 이미지와 모든 텍스트 프롬프트를 각각 CLIP의 이미지 인코더와 텍스트 인코더에 통과시켜 이미지 임베딩과 텍스트 임베딩을 추출합니다.\n3. **유사도 계산**: 이미지 임베딩과 각 텍스트 임베딩 간의 코사인 유사도(cosine similarity)를 계산합니다.\n4. **분류**: 계산된 유사도 점수가 가장 높은 텍스트 프롬프트에 해당하는 클래스를 최종 예측 결과로 선택합니다. 이 과정은 모델이 해당 클래스를 미리 학습하지 않았음에도 가능합니다.",
        "keywords_full_credit": ["CLIP", "제로샷 분류", "이미지 임베딩", "텍스트 임베딩", "코사인 유사도"],
        "keywords_partial_credit": ["텍스트 프롬프트", "인코더"]
    },
    {
        "subject": "[실습] RAG",
        "question_text": "RAG(Retrieval-Augmented Generation) 시스템의 주요 구성요소 3가지(Retriever, Generator, Knowledge Source)와 각 구성요소의 역할을 설명하고, 전체적인 동작 과정을 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "RAG 시스템의 주요 구성 요소는 다음과 같습니다.\n1. **Knowledge Source**: 최신 정보를 담고 있는 외부 문서 데이터베이스(예: Wikipedia, 사내 문서)입니다.\n2. **Retriever**: 사용자의 질문이 들어오면, Knowledge Source에서 해당 질문과 가장 관련성이 높은 문서를 검색하여 찾는 역할을 합니다.\n3. **Generator**: 검색된 문서 내용과 사용자의 질문을 함께 입력받아, 이를 바탕으로 최종 답변을 생성하는 LLM입니다.\n전체 동작 과정은 사용자의 질문을 Retriever가 받아 관련 문서를 검색하고, 이 검색된 문서와 질문을 Generator에 함께 전달하여 풍부한 정보를 바탕으로 정확하고 상세한 답변을 생성하는 방식입니다.",
        "keywords_full_credit": ["RAG", "Retriever", "Generator", "Knowledge Source"],
        "keywords_partial_credit": ["검색", "생성", "외부 문서"]
    },
    {
    "subject": "[실습] RAG",
    "question_text": "LangChain Expression Language (LCEL)의 개념과 파이프(|) 연산자를 사용하여 여러 컴포넌트를 연결하는 방식의 장점에 대해 설명하시오. Retriever, Prompt Template, LLM을 LCEL로 연결하여 질문에 답변하는 체인(chain)을 만드는 예시 코드를 작성하시오.",
    "question_type": "descriptive",
    "model_answer": "LCEL은 LangChain의 구성 요소(retriever, prompt, LLM 등)를 파이프(|) 연산자로 순서대로 연결해서 체인처럼 실행할 수 있게 해주는 표현 언어이다. 이 방식은 각 단계의 입력과 출력을 명확하게 선언식으로 구성할 수 있고, 리트리버 → 프롬프트 템플릿 → LLM → 후처리 파서를 한 흐름으로 표현할 수 있다는 장점이 있다. 또한 중간 단계의 값을 키로 넘기거나(assign), 병렬로 값을 준비하는 등의 작업을 코드 몇 줄로 작성할 수 있어 유지보수성이 높다.\n\n예시:\n\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_upstage import ChatUpstage\nfrom operator import itemgetter\n\n# retriever, rag_prompt, llm 이 이미 정의되어 있다고 가정한다.\n# llm = ChatUpstage()\n# def format_docs(docs):\n#     return \"\\n\\n\".join(doc.page_content for doc in docs)\n\nrag_chain = (\n    RunnablePassthrough.assign(\n        context = itemgetter(\"question\") | retriever | format_docs,\n        question = itemgetter(\"question\")\n    )\n    | rag_prompt\n    | llm\n    | StrOutputParser()\n)\n\nresult = rag_chain.invoke({\"question\": \"주말에도 배송되나요?\"})",
    "keywords_full_credit": [
        "LCEL",
        "파이프 연산자",
        "RunnablePassthrough",
        "assign"
    ],
    "keywords_partial_credit": [
        "LangChain",
        "체인",
        "retriever",
        "prompt",
        "LLM"
    ]
},
    {
        "subject": "[실습] Python for AI",
        "question_text": "Python의 `collections.Counter` 객체는 어떤 목적으로 사용되며, 리스트 내 원소들의 빈도를 계산하는 예시 코드를 작성하고, 가장 빈번하게 나타나는 상위 2개 원소를 찾는 방법을 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "`collections.Counter`는 해시 가능한(hashable) 객체들의 개수를 세는 데 사용되는 딕셔너리 서브클래스입니다. 리스트나 문자열과 같은 이터러블(iterable) 객체를 입력으로 받아 각 원소가 몇 번 나타나는지를 키-값 쌍으로 저장합니다.\n```python\nfrom collections import Counter\n\nfruits = ['apple', 'banana', 'apple', 'orange', 'banana', 'banana']\nfruit_counts = Counter(fruits)\nprint(fruit_counts)\n# Counter({'banana': 3, 'apple': 2, 'orange': 1})\n\n# 가장 빈번한 상위 2개 원소 찾기\ntop_two = fruit_counts.most_common(2)\nprint(top_two)\n# [('banana', 3), ('apple', 2)]\n```\n가장 빈번한 원소를 찾기 위해서는 `most_common(n)` 메서드를 사용하며, `n`은 가져올 상위 원소의 개수를 의미합니다.",
        "keywords_full_credit": ["collections.Counter", "빈도 계산", "most_common"],
        "keywords_partial_credit": ["이터러블", "딕셔너리 서브클래스"]
    },
    {
        "subject": "[실습] EDA",
        "question_text": "데이터 분석에서 범주형(Categorical) 데이터를 처리하는 것이 왜 중요한지 설명하고, pandas의 `get_dummies` 함수를 사용하여 원-핫 인코딩(One-Hot Encoding)을 수행하는 방법을 코드 예시와 함께 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "대부분의 머신러닝 모델은 수치형 데이터를 입력으로 받기 때문에, '성별', '요일'과 같은 범주형 데이터를 모델이 이해할 수 있는 숫자 형태로 변환하는 과정이 필수적입니다. 원-핫 인코딩은 각 범주를 새로운 컬럼으로 만들고, 해당 범주에 속하면 1, 아니면 0으로 표시하여 범주 간의 순서나 관계가 없음을 명확히 나타내는 대표적인 방법입니다.\n```python\nimport pandas as pd\ndf = pd.DataFrame({'color': ['red', 'blue', 'green', 'red']})\ndf_encoded = pd.get_dummies(df, columns=['color'])\nprint(df_encoded)\n```\n위 코드는 'color' 컬럼을 'color_red', 'color_blue', 'color_green' 세 개의 새로운 이진(binary) 컬럼으로 변환합니다.",
        "keywords_full_credit": ["범주형 데이터", "수치형 변환", "원-핫 인코딩", "get_dummies"],
        "keywords_partial_credit": ["머신러닝 모델", "순서 관계 없음"]
    },
    {
        "subject": "[실습] MLP",
        "question_text": "PyTorch에서 `optimizer.zero_grad()`, `loss.backward()`, `optimizer.step()`의 역할과 호출 순서의 중요성에 대해 각각 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "이 세 함수는 모델의 가중치를 업데이트하는 핵심적인 과정입니다.\n1. `optimizer.zero_grad()`: 이전 배치의 그래디언트가 현재 배치에 누적되는 것을 방지하기 위해, 모든 모델 파라미터의 그래디언트를 0으로 초기화합니다. 반드시 `loss.backward()` 전에 호출되어야 합니다.\n2. `loss.backward()`: 계산된 손실(loss)에 대해 역전파를 수행하여 각 파라미터에 대한 그래디언트를 계산합니다.\n3. `optimizer.step()`: `backward()`를 통해 계산된 그래디언트를 사용하여 옵티마이저(예: Adam, SGD)가 모델의 가중치를 업데이트합니다. 반드시 `loss.backward()` 이후에 호출되어야 합니다.\n이 순서(초기화 -> 그래디언트 계산 -> 가중치 업데이트)는 매 학습 반복마다 정확한 가중치 업데이트를 보장하기 위해 반드시 지켜져야 합니다.",
        "keywords_full_credit": ["optimizer.zero_grad()", "loss.backward()", "optimizer.step()", "그래디언트 초기화", "역전파", "가중치 업데이트"],
        "keywords_partial_credit": ["호출 순서", "그래디언트 누적 방지"]
    },
    {
        "subject": "[실습] MLP",
        "question_text": "딥러닝 모델의 과적합(Overfitting)을 방지하기 위한 기법인 '조기 종료(Early Stopping)'의 개념과 구현 방법에 대해 설명하시오. 검증 손실(validation loss)은 이 과정에서 어떤 역할을 합니까?",
        "question_type": "descriptive",
        "model_answer": "조기 종료는 모델이 훈련 데이터에 과도하게 최적화되어 검증 데이터에 대한 성능이 더 이상 개선되지 않고 오히려 저하되기 시작할 때 학습을 중단하는 기법입니다. 구현을 위해서는 매 에포크(epoch)마다 검증 데이터셋으로 손실(validation loss)을 계산하고, 이 손실이 이전의 최저 손실보다 개선되지 않는 에포크 횟수(patience)를 셉니다. 만약 이 횟수가 미리 정해둔 최대 허용 횟수에 도달하면, 모델 성능이 정점에 도달했다고 판단하고 학습을 멈춥니다. 즉, 검증 손실은 모델의 일반화 성능을 모니터링하고 학습 중단 시점을 결정하는 핵심 지표 역할을 합니다.",
        "keywords_full_credit": ["조기 종료", "과적합 방지", "검증 손실(validation loss)", "patience"],
        "keywords_partial_credit": ["일반화 성능", "학습 중단"]
    },
    {
        "subject": "[실습] Python for AI",
        "question_text": "Python의 `*args`와 `**kwargs`의 차이점과 각각의 사용법을 설명하고, 이들을 함께 사용하는 함수 예시를 작성하시오.",
        "question_type": "descriptive",
        "model_answer": "`*args`는 함수에 전달되는 임의의 개수의 위치 인자(positional arguments)들을 튜플 형태로 받습니다. `**kwargs`는 임의의 개수의 키워드 인자(keyword arguments)들을 딕셔너리 형태로 받습니다. 이 둘을 함께 사용하면 함수가 매우 유연하게 다양한 종류의 인자를 받을 수 있습니다.\n```python\ndef process_data(*args, **kwargs):\n    print(\"Positional arguments:\", args)\n    print(\"Keyword arguments:\", kwargs)\n\nprocess_data(1, 'hello', 3.14, name='Alice', age=30)\n# Positional arguments: (1, 'hello', 3.14)\n# Keyword arguments: {'name': 'Alice', 'age': 30}\n```",
        "keywords_full_credit": ["*args", "**kwargs", "위치 인자", "키워드 인자"],
        "keywords_partial_credit": ["튜플", "딕셔너리"]
    },
    {
        "subject": "[실습] EDA",
        "question_text": "Pandas DataFrame에서 `loc`와 `iloc` 인덱서의 차이점을 설명하고, 각각을 사용하여 특정 행과 열을 선택하는 예시 코드를 작성하시오.",
        "question_type": "descriptive",
        "model_answer": "`loc`는 레이블(label) 기반 인덱싱을 사용하여 데이터를 선택합니다. 즉, 행과 열의 이름(레이블)을 직접 사용합니다. 반면, `iloc`는 정수 위치(integer-location) 기반 인덱싱을 사용하여, 0부터 시작하는 정수 인덱스로 데이터를 선택합니다.\n```python\nimport pandas as pd\ndf = pd.DataFrame({'A': [10, 20, 30], 'B': [40, 50, 60]}, index=['row1', 'row2', 'row3'])\n\n# loc 예시: 'row2' 행의 'B' 열 선택\nprint(df.loc['row2', 'B'])\n# 50\n\n# iloc 예시: 1번 인덱스 행의 0번 인덱스 열 선택\nprint(df.iloc[1, 0])\n# 20\n```",
        "keywords_full_credit": ["loc", "iloc", "레이블 기반", "정수 위치 기반"],
        "keywords_partial_credit": ["인덱서", "DataFrame"]
    },
    {
        "subject": "[실습] Linear Regression",
        "question_text": "로지스틱 회귀(Logistic Regression)가 분류 문제에 어떻게 사용되는지 시그모이드(Sigmoid) 함수와 연관지어 설명하고, 선형 회귀를 분류 문제에 직접 사용하기 어려운 이유를 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "로지스틱 회귀는 선형 회귀의 예측값(음의 무한대에서 양의 무한대)을 시그모이드 함수에 통과시켜 0과 1 사이의 확률 값으로 변환합니다. 이 확률 값을 기준으로 특정 임계값(보통 0.5)을 넘으면 클래스 1, 넘지 않으면 클래스 0으로 분류합니다. 선형 회귀를 분류에 직접 사용하기 어려운 이유는 예측값이 [0, 1] 범위를 벗어날 수 있어 확률로 해석하기 어렵고, 이상치(outlier)에 매우 민감하여 결정 경계(decision boundary)가 쉽게 왜곡될 수 있기 때문입니다.",
        "keywords_full_credit": ["로지스틱 회귀", "시그모이드 함수", "확률 변환", "분류"],
        "keywords_partial_credit": ["선형 회귀", "결정 경계"]
    },
    {
        "subject": "[실습] MLP",
        "question_text": "PyTorch에서 `nn.Module`을 상속받아 사용자 정의 모델을 만들 때, `__init__` 메서드와 `forward` 메서드의 역할에 대해 각각 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "`__init__` 메서드는 모델을 생성할 때 한 번만 호출되는 생성자(constructor)입니다. 이 메서드 안에서 모델에 필요한 레이어(예: `nn.Linear`, `nn.Conv2d`, `nn.ReLU`)들을 정의하고 초기화합니다. `forward` 메서드는 모델의 순전파(forward pass) 로직을 정의하는 곳입니다. 입력 데이터가 모델에 들어왔을 때, `__init__`에서 정의한 레이어들을 어떤 순서로 거쳐 최종 출력을 만들어낼지를 결정합니다. 모델 객체를 함수처럼 호출하면 내부적으로 이 `forward` 메서드가 실행됩니다.",
        "keywords_full_credit": ["nn.Module", "__init__", "forward", "레이어 정의", "순전파 로직"],
        "keywords_partial_credit": ["생성자", "초기화"]
    },
    {
        "subject": "[실습] Tokenizer & Embedding",
        "question_text": "자연어 처리에서 사용되는 토큰화(Tokenization) 기법 중, BPE(Byte Pair Encoding)의 동작 원리를 단계별로 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "BPE는 데이터 압축 알고리즘에서 유래한 토큰화 기법으로, 가장 빈번하게 등장하는 문자 쌍(byte pair)을 새로운 하나의 문자로 병합하는 과정을 반복합니다. 동작 원리는 다음과 같습니다.\n1. **초기화**: 모든 단어를 문자(character) 단위로 분리하고, 초기 단어장(vocabulary)을 구성합니다.\n2. **반복**: 훈련 데이터에서 가장 빈번하게 연속적으로 등장하는 문자 쌍을 찾습니다.\n3. **병합**: 찾은 문자 쌍을 하나의 새로운 토큰(subword)으로 병합하고, 이 토큰을 단어장에 추가합니다.\n4. **종료**: 미리 정해둔 반복 횟수나 단어장 크기에 도달할 때까지 2-3번 과정을 반복합니다. 이를 통해 자주 등장하는 단어는 그대로 유지되고, 희귀한 단어는 의미 있는 부분 단어(subword)들로 분리됩니다.",
        "keywords_full_credit": ["BPE", "Byte Pair Encoding", "병합", "단어장(vocabulary)", "subword"],
        "keywords_partial_credit": ["토큰화", "문자 쌍"]
    },
    {
        "subject": "[실습] CNN",
        "question_text": "CNN(Convolutional Neural Network)의 핵심 구성 요소인 '컨볼루션 레이어(Convolution Layer)'와 '풀링 레이어(Pooling Layer)'의 역할과 동작 방식에 대해 각각 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "**컨볼루션 레이어**는 필터(또는 커널)를 입력 이미지 위에서 이동시키면서 합성곱(convolution) 연산을 수행하여 특징 맵(feature map)을 생성합니다. 필터는 이미지의 특정 패턴(예: 엣지, 질감)을 감지하는 역할을 하며, 이 과정을 통해 이미지의 공간적 특징을 추출합니다. **풀링 레이어**는 컨볼루션 레이어의 출력인 특징 맵의 크기를 줄이는 역할을 합니다. 주로 최대 풀링(Max Pooling)이 사용되며, 특정 영역에서 가장 큰 값만 남기고 나머지는 버립니다. 이를 통해 계산량을 줄이고, 모델이 작은 위치 변화에 덜 민감하게 만들어(translation invariance) 강건성을 높입니다.",
        "keywords_full_credit": ["컨볼루션 레이어", "풀링 레이어", "특징 맵(feature map)", "차원 축소"],
        "keywords_partial_credit": ["필터", "합성곱", "최대 풀링"]
    },
    {
        "subject": "[실습] Image Generate",
        "question_text": "생성 모델(Generative Model)의 성능을 평가하는 지표 중, IS(Inception Score)와 FID(Fréchet Inception Distance)의 차이점을 설명하고, FID가 IS에 비해 가지는 장점을 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "IS는 생성된 이미지의 품질(선명도)과 다양성을 측정하지만, 실제 이미지 데이터셋의 분포와 비교하지는 않습니다. 반면, FID는 사전 학습된 Inception 모델을 사용하여 생성된 이미지와 실제 이미지의 특징 벡터(feature vector) 분포를 각각 추출하고, 두 분포 간의 거리(Fréchet Inception Distance)를 계산합니다. FID는 실제 이미지 분포와의 유사성을 직접적으로 비교하기 때문에, 생성된 이미지의 품질과 다양성뿐만 아니라 사실성까지 더 잘 반영할 수 있다는 장점이 있습니다. FID 점수가 낮을수록 생성된 이미지가 실제 이미지와 더 유사함을 의미합니다.",
        "keywords_full_credit": ["FID", "Inception Score", "실제 이미지 분포 비교", "특징 벡터"],
        "keywords_partial_credit": ["생성 모델 평가", "품질", "다양성", "사실성"]
    },
    {
        "subject": "[실습] RAG",
        "question_text": "LangChain에서 에이전트(Agent)가 도구(Tool)를 사용하는 과정을 설명하고, `create_retriever_tool` 함수를 사용하여 Retriever를 도구로 만들 때 `name`과 `description` 파라미터가 왜 중요한지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "LangChain 에이전트는 LLM을 핵심 엔진으로 사용하여, 사용자의 복잡한 요청을 해결하기 위해 어떤 도구를 어떤 순서로 사용해야 할지 스스로 결정하고 실행합니다. 이 과정에서 `name`은 도구를 식별하는 고유한 이름으로 사용되고, `description`은 도구의 기능과 용도를 자연어로 설명하는 역할을 합니다. 에이전트(LLM)는 이 `description`을 읽고 현재 문제 상황에 가장 적합한 도구가 무엇인지 판단하기 때문에, 명확하고 상세한 설명은 에이전트가 올바른 결정을 내리는 데 매우 중요합니다.",
        "keywords_full_credit": ["에이전트(Agent)", "도구(Tool)", "name", "description"],
        "keywords_partial_credit": ["LangChain", "create_retriever_tool", "LLM"]
    },
    {
        "subject": "[실습] PEFT",
        "question_text": "LoRA 파인튜닝에서 `r` (rank)과 `lora_alpha` 하이퍼파라미터의 역할을 각각 설명하고, 이 두 값의 관계에 대해 서술하시오.",
        "question_type": "descriptive",
        "model_answer": "`r`은 LoRA에서 추가되는 저랭크 행렬 A와 B의 랭크(차원)를 결정합니다. `r`이 클수록 더 많은 파라미터가 학습되어 더 복잡한 패턴을 학습할 수 있지만, 메모리 사용량과 계산 비용이 증가합니다. `lora_alpha`는 학습된 LoRA 가중치의 스케일링(scaling)을 조절하는 역할을 합니다. 일반적으로 `lora_alpha`를 `r`의 두 배(`alpha = 2 * r`)로 설정하는 것이 좋은 성능을 내는 경험적인 규칙으로 알려져 있으며, 이는 학습된 가중치의 영향력을 적절히 조절하는 역할을 합니다.",
        "keywords_full_credit": ["LoRA", "r(rank)", "lora_alpha", "스케일링"],
        "keywords_partial_credit": ["하이퍼파라미터", "저랭크 행렬"]
    },
    {
        "subject": "[실습] Data Augmentation",
        "question_text": "LLM을 사용하여 합성 데이터를 생성할 때, 결과물의 '다양성'과 '일관성'을 조절하는 주요 파라미터인 `temperature`와 `top_p`에 대해 각각 설명하시오. 창의적인 아이디어를 많이 얻고 싶을 때와 예측 가능한 답변을 얻고 싶을 때 이 값들을 어떻게 설정해야 하는지 설명하시오.",
        "question_type": "descriptive",
        "model_answer": "`temperature`는 다음 토큰을 선택할 때 모델의 확률 분포를 얼마나 부드럽게(또는 뾰족하게) 만들지를 조절합니다. 값이 높을수록 확률이 낮은 토큰도 선택될 가능성이 높아져 더 다양하고 창의적인 결과가 나옵니다. `top_p` (Nucleus Sampling)는 누적 확률이 `p`가 될 때까지의 상위 토큰들 중에서만 다음 토큰을 샘플링하는 방식입니다. 창의적인 아이디어를 원할 때는 `temperature`를 높이고(예: 0.8 이상) `top_p`도 비교적 높게(예: 0.95) 설정합니다. 반면, 사실에 기반한 예측 가능한 답변을 원할 때는 `temperature`를 낮추고(예: 0.2) `top_p`도 낮게 설정하여 가장 확률 높은 토큰 위주로 선택되도록 합니다.",
        "keywords_full_credit": ["temperature", "top_p", "다양성", "일관성", "샘플링"],
        "keywords_partial_credit": ["확률 분포", "Nucleus Sampling"]
    }
]