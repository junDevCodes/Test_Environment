[
    {
        "subject": "[이론] Machine Learning",
        "question_text": "다음 중 지도 학습(Supervised Learning)에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["정답지가 없는 데이터를 사용하여 모델을 학습시킨다.", "보상과 벌을 통해 최적의 행동을 학습한다.", "연속적인 숫자 값을 예측하는 회귀와 특정 카테고리를 분류하는 문제에 사용된다.", "데이터의 숨겨진 구조나 패턴을 찾는 데 중점을 둔다."],
        "model_answer": "연속적인 숫자 값을 예측하는 회귀와 특정 카테고리를 분류하는 문제에 사용된다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Machine Learning",
        "question_text": "로지스틱 회귀(Logistic Regression)에 대한 설명으로 틀린 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["주로 이진 분류 문제에 사용된다.", "선형 회귀 모델의 예측값에 시그모이드 함수를 적용하여 확률을 계산한다.", "다중 클래스 분류를 위해 소프트맥스 함수를 사용할 수 있다.", "연속적인 값을 예측하는 회귀 문제에 가장 적합하다."],
        "model_answer": "연속적인 값을 예측하는 회귀 문제에 가장 적합하다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Gradient Descent",
        "question_text": "경사 하강법(Gradient Descent)에서 '학습률(Learning Rate)'의 역할로 가장 적절한 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["전체 데이터셋을 몇 번이나 반복하여 학습할지를 결정한다.", "한 번의 학습 단계에서 가중치를 얼마나 업데이트할지를 결정하는 보폭(step size) 역할을 한다.", "학습을 언제 중단할지를 결정하는 오차의 임계값이다.", "한 번에 몇 개의 데이터를 사용하여 그래디언트를 계산할지를 결정한다."],
        "model_answer": "한 번의 학습 단계에서 가중치를 얼마나 업데이트할지를 결정하는 보폭(step size) 역할을 한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] EDA",
        "question_text": "데이터 시각화에서 이상치(Outlier)를 탐지하는 데 가장 효과적인 그래프는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["히스토그램 (Histogram)", "박스 플롯 (Box Plot)", "산점도 (Scatter Plot)", "히트맵 (Heatmap)"],
        "model_answer": "박스 플롯 (Box Plot)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Model Evaluation",
        "question_text": "모델 평가 지표 중, 실제 Positive인 것들 중에서 모델이 Positive라고 예측한 것의 비율을 나타내는 것은 무엇입니까? (암 환자 진단과 같이 실제 환자를 놓치면 안 되는 경우에 중요합니다.)",
        "question_type": "multiple_choice",
        "options": ["정확도 (Accuracy)", "정밀도 (Precision)", "재현율 (Recall)", "F1 점수 (F1 Score)"],
        "model_answer": "재현율 (Recall)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "MLP(다중 퍼셉트론)의 역전파(Backpropagation) 과정에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["입력 데이터가 은닉층을 거쳐 출력층으로 전달되며 예측값을 계산하는 과정이다.", "모델의 과적합을 막기 위해 일부 뉴런을 랜덤하게 비활성화시키는 과정이다.", "예측값과 실제값의 오차를 기반으로 출력층에서부터 입력층 방향으로 가중치를 업데이트하는 과정이다.", "가중치를 무작위로 초기화하여 학습을 시작하는 과정이다."],
        "model_answer": "예측값과 실제값의 오차를 기반으로 출력층에서부터 입력층 방향으로 가중치를 업데이트하는 과정이다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "활성화 함수 ReLU(Rectified Linear Unit)의 주요 특징으로 옳은 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["입력값이 0보다 작으면 -1을, 크면 1을 출력하여 항상 일정한 출력을 보장한다.", "입력값이 0보다 작으면 0을 출력하여 기울기 소실(Vanishing Gradient) 문제를 완화한다.", "모든 입력값에 대해 항상 0과 1 사이의 값을 출력하여 확률적 해석을 가능하게 한다.", "계산이 복잡하지만, 시그모이드 함수보다 항상 더 나은 성능을 보인다."],
        "model_answer": "입력값이 0보다 작으면 0을 출력하여 기울기 소실(Vanishing Gradient) 문제를 완화한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "다음 중 Adam 옵티마이저에 대한 설명으로 가장 적절한 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["모든 가중치를 동일한 학습률로 업데이트하는 가장 기본적인 최적화 알고리즘이다.", "과거의 그래디언트 방향을 참고하는 '관성'과 적응적인 학습률 조정을 결합한 방식이다.", "학습 과정에서 일부 가중치를 0으로 만들어 모델을 희소(sparse)하게 만든다.", "전체 데이터셋을 사용해야만 가중치를 업데이트할 수 있다."],
        "model_answer": "과거의 그래디언트 방향을 참고하는 '관성'과 적응적인 학습률 조정을 결합한 방식이다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] CNN",
        "question_text": "CNN(합성곱 신경망)에서 풀링(Pooling) 계층의 주된 역할은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["이미지의 특징을 추출하기 위해 필터 연산을 수행한다.", "추출된 특징 맵(Feature Map)의 크기를 줄여 계산 효율성을 높이고 과적합을 방지한다.", "비선형성을 추가하여 모델의 표현력을 높인다.", "최종적으로 이미지를 분류하기 위해 모든 특징을 종합한다."],
        "model_answer": "추출된 특징 맵(Feature Map)의 크기를 줄여 계산 효율성을 높이고 과적합을 방지한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] RNN",
        "question_text": "RNN(순환 신경망)이 기존의 신경망과 구별되는 가장 큰 특징은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["여러 개의 은닉층을 깊게 쌓아 복잡한 패턴을 학습한다.", "이미지 데이터 처리에 특화된 필터와 풀링 연산을 사용한다.", "이전 시점의 정보를 기억하는 '은닉 상태(hidden state)'를 통해 순차적인 데이터를 처리한다.", "정답이 없는 데이터로부터 스스로 특징을 학습하는 비지도 학습 방식이다."],
        "model_answer": "이전 시점의 정보를 기억하는 '은닉 상태(hidden state)'를 통해 순차적인 데이터를 처리한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] RNN",
        "question_text": "LSTM(Long Short-Term Memory)이 기존 RNN의 장기 의존성(Long-Term Dependency) 문제를 해결하기 위해 도입한 핵심적인 메커니즘은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["드롭아웃(Dropout)을 통해 뉴런을 무작위로 비활성화한다.", "어텐션(Attention) 메커니즘을 통해 입력의 특정 부분에 집중한다.", "셀 상태(Cell State)와 여러 게이트(Forget, Input, Output)를 사용하여 정보의 흐름을 제어한다.", "합성곱(Convolution) 연산을 통해 지역적인 특징을 추출한다."],
        "model_answer": "셀 상태(Cell State)와 여러 게이트(Forget, Input, Output)를 사용하여 정보의 흐름을 제어한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] NLP",
        "question_text": "트랜스포머(Transformer) 모델의 셀프 어텐션(Self-Attention) 메커니즘에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["순차적인 데이터 처리를 위해 재귀적인 구조를 사용한다.", "문장 내 단어들 간의 관계를 한 번에 병렬적으로 계산하여 문맥을 파악한다.", "이미지의 지역적인 특징을 추출하는 데 주로 사용된다.", "이전 단어의 정보만을 사용하여 다음 단어를 예측한다."],
        "model_answer": "문장 내 단어들 간의 관계를 한 번에 병렬적으로 계산하여 문맥을 파악한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] NLP",
        "question_text": "BERT 모델이 양방향 문맥을 학습하기 위해 사용하는 주요 학습 방법 두 가지는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["다음 문장 예측(NSP)과 마스크된 언어 모델(MLM)", "감성 분석과 개체명 인식", "기계 번역과 텍스트 요약", "오토인코더와 생성적 적대 신경망(GAN)"],
        "model_answer": "다음 문장 예측(NSP)과 마스크된 언어 모델(MLM)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] CNN",
        "question_text": "ResNet(Residual Network) 아키텍처가 매우 깊은 신경망을 효과적으로 학습시킬 수 있는 핵심적인 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["드롭아웃을 사용하여 과적합을 방지하기 때문", "배치 정규화를 통해 학습을 안정화시키기 때문", "잔차 연결(Residual Connection)을 통해 기울기 소실 문제를 완화하기 때문", "합성곱 필터의 크기를 동적으로 조절하기 때문"],
        "model_answer": "잔차 연결(Residual Connection)을 통해 기울기 소실 문제를 완화하기 때문",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] CNN",
        "question_text": "Vision Transformer(ViT) 모델이 이미지를 처리하는 방식에 대한 설명으로 가장 적절한 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["이미지를 여러 개의 작은 패치로 나누고, 이를 시퀀스 데이터처럼 트랜스포머 인코더에 입력한다.", "이미지 전체에 대해 단일 합성곱 필터를 적용하여 특징을 추출한다.", "순환 신경망을 사용하여 이미지의 픽셀을 순차적으로 처리한다.", "이미지의 각 픽셀을 독립적인 데이터 포인트로 간주하여 처리한다."],
        "model_answer": "이미지를 여러 개의 작은 패치로 나누고, 이를 시퀀스 데이터처럼 트랜스포머 인코더에 입력한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "검색 증강 생성(RAG) 모델이 기존 LLM의 환각(Hallucination) 현상을 완화하는 주된 원리는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["모델의 파라미터 수를 대폭 늘려 지식의 양을 증가시킨다.", "답변 생성 시, 외부의 신뢰할 수 있는 지식 소스에서 검색한 정보를 근거로 활용한다.", "더 많은 데이터셋으로 모델을 추가 학습시켜 잘못된 정보를 수정한다.", "사용자의 질문 의도를 더 잘 파악하기 위해 복잡한 프롬프트를 사용한다."],
        "model_answer": "답변 생성 시, 외부의 신뢰할 수 있는 지식 소스에서 검색한 정보를 근거로 활용한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "PEFT(Parameter-Efficient Fine-Tuning)의 주된 목적은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["사전 학습된 모델의 모든 파라미터를 미세 조정하여 최고 성능을 달성하는 것", "적은 수의 파라미터만 업데이트하여 계산 비용과 메모리 사용량을 줄이면서 모델을 특정 작업에 맞게 조정하는 것", "모델의 크기를 줄이기 위해 중요하지 않은 가중치를 제거하는 것", "모델의 추론 속도를 높이기 위해 가중치를 낮은 정밀도로 변환하는 것"],
        "model_answer": "적은 수의 파라미터만 업데이트하여 계산 비용과 메모리 사용량을 줄이면서 모델을 특정 작업에 맞게 조정하는 것",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LoRA(Low-Rank Adaptation) 기법에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["기존 모델의 가중치는 고정한 채, 학습 가능한 저차원 행렬을 추가하여 파라미터 업데이트를 효율화한다.", "모델의 마지막 레이어만 재학습하여 새로운 작업에 적응시킨다.", "모델의 모든 레이어에 작은 노이즈를 추가하여 일반화 성능을 높인다.", "학습 데이터에 없는 새로운 단어를 처리하기 위해 어휘 사전을 확장한다."],
        "model_answer": "기존 모델의 가중치는 고정한 채, 학습 가능한 저차원 행렬을 추가하여 파라미터 업데이트를 효율화한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "모델 양자화(Quantization) 기술의 주된 이점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["모델의 정확도를 항상 향상시킨다.", "모델의 파라미터를 더 낮은 정밀도로 표현하여 모델 크기와 연산량을 줄인다.", "모델이 다양한 언어를 이해할 수 있도록 돕는다.", "모델의 학습 과정을 안정화시킨다."],
        "model_answer": "모델의 파라미터를 더 낮은 정밀도로 표현하여 모델 크기와 연산량을 줄인다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "다음 중 LLM의 '창발적 능력(Emergent Property)'에 해당하는 예시로 가장 적절한 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["간단한 문법 오류를 수정하는 능력", "모델의 규모가 특정 임계점을 넘었을 때, 별도의 학습 없이 복잡한 추론이나 문맥 기반 학습(In-Context Learning)을 수행하는 능력", "입력된 텍스트의 다음 단어를 예측하는 능력", "학습 데이터에 포함된 특정 사실을 기억하는 능력"],
        "model_answer": "모델의 규모가 특정 임계점을 넘었을 때, 별도의 학습 없이 복잡한 추론이나 문맥 기반 학습(In-Context Learning)을 수행하는 능력",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] NLP",
        "question_text": "Word2Vec의 CBOW(Continuous Bag of Words) 모델과 Skip-gram 모델의 차이점으로 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["CBOW는 중심 단어로 주변 단어를 예측하고, Skip-gram은 주변 단어들로 중심 단어를 예측한다.", "CBOW는 주변 단어들로 중심 단어를 예측하고, Skip-gram은 중심 단어로 주변 단어를 예측한다.", "CBOW는 단어의 순서를 고려하지만, Skip-gram은 고려하지 않는다.", "CBOW는 희귀 단어 표현에 강하고, Skip-gram은 학습 속도가 빠르다."],
        "model_answer": "CBOW는 주변 단어들로 중심 단어를 예측하고, Skip-gram은 중심 단어로 주변 단어를 예측한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Model Evaluation",
        "question_text": "데이터 불균형이 심한 분류 문제에서 모델 성능을 평가할 때, 정확도(Accuracy)보다 F1 점수(F1-Score)가 더 선호되는 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["F1 점수는 계산이 더 간단하기 때문이다.", "F1 점수는 정밀도(Precision)와 재현율(Recall)의 조화 평균으로, 두 지표를 모두 고려하여 클래스 불균형에 더 강건하기 때문이다.", "F1 점수는 항상 정확도보다 높은 값을 가지기 때문이다.", "F1 점수는 모델의 학습 속도를 나타내는 지표이기 때문이다."],
        "model_answer": "F1 점수는 정밀도(Precision)와 재현율(Recall)의 조화 평균으로, 두 지표를 모두 고려하여 클래스 불균형에 더 강건하기 때문이다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "과적합(Overfitting)을 방지하기 위한 규제(Regularization) 기법이 아닌 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["L1 규제 (Lasso)", "L2 규제 (Ridge)", "드롭아웃 (Dropout)", "학습률 증가 (Increasing Learning Rate)"],
        "model_answer": "학습률 증가 (Increasing Learning Rate)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] CNN",
        "question_text": "CNN에서 합성곱(Convolution) 연산의 출력 크기를 계산하는 공식으로 옳은 것은 무엇입니까? (입력 크기: W, 필터 크기: K, 패딩: P, 스트라이드: S)",
        "question_type": "multiple_choice",
        "options": ["(W - K + 2P) / S + 1", "(W + K - 2P) / S + 1", "(W - K - 2P) * S + 1", "(W + K + 2P) / S - 1"],
        "model_answer": "(W - K + 2P) / S + 1",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM의 디코딩 전략 중, 항상 확률이 가장 높은 다음 토큰을 선택하는 방식은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Greedy Decoding", "Beam Search", "Top-K Sampling", "Top-P Sampling"],
        "model_answer": "Greedy Decoding",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "Top-P (Nucleus) 샘플링 방식에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["확률이 가장 높은 K개의 토큰 중에서만 샘플링한다.", "누적 확률이 특정 임계값 P를 넘는 최소한의 토큰 집합에서 샘플링한다.", "모든 토큰을 대상으로 확률에 비례하여 랜덤하게 샘플링한다.", "가장 확률이 높은 단 하나의 토큰만을 선택한다."],
        "model_answer": "누적 확률이 특정 임계값 P를 넘는 최소한의 토큰 집합에서 샘플링한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "RLHF(인간 피드백을 통한 강화학습)의 주요 단계가 아닌 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["사전 훈련된 언어 모델 준비", "사람의 선호도 데이터를 수집하여 보상 모델(Reward Model) 학습", "보상 모델을 사용하여 강화학습으로 언어 모델 미세 조정", "모델의 모든 가중치를 무작위로 재초기화"],
        "model_answer": "모델의 모든 가중치를 무작위로 재초기화",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG 시스템에서 텍스트를 의미 있는 작은 단위로 나누는 '청킹(Chunking)'을 수행하는 주된 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["텍스트의 전체 길이를 늘리기 위해", "LLM의 컨텍스트 창(Context Window) 한계를 극복하고 관련성 높은 정보 검색을 용이하게 하기 위해", "텍스트를 암호화하여 보안을 강화하기 위해", "텍스트에 포함된 모든 단어의 빈도를 계산하기 위해"],
        "model_answer": "LLM의 컨텍스트 창(Context Window) 한계를 극복하고 관련성 높은 정보 검색을 용이하게 하기 위해",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG 파이프라인에서 'Retriever'의 역할은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["사용자의 질문을 LLM이 이해할 수 있는 형태로 변환한다.", "Vector Store에서 사용자의 질문과 의미적으로 가장 유사한 문서 청크를 검색한다.", "LLM이 생성한 답변을 최종 사용자에게 전달한다.", "문서 청크를 벡터로 변환하는 임베딩 모델을 학습시킨다."],
        "model_answer": "Vector Store에서 사용자의 질문과 의미적으로 가장 유사한 문서 청크를 검색한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Machine Learning",
        "question_text": "비지도 학습(Unsupervised Learning)의 대표적인 예시로 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["주택 가격 예측", "스팸 메일 분류", "고객 데이터 군집화(Clustering)", "이미지 속 객체 인식"],
        "model_answer": "고객 데이터 군집화(Clustering)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Data Preprocessing",
        "question_text": "데이터 전처리에서 표준화(Standardization)와 정규화(Normalization)의 주된 차이점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["표준화는 데이터를 0과 1 사이로 조정하고, 정규화는 평균 0, 표준편차 1로 조정한다.", "표준화는 평균 0, 표준편차 1로 조정하고, 정규화는 데이터를 0과 1 사이로 조정한다.", "표준화는 범주형 데이터에, 정규화는 수치형 데이터에 사용된다.", "표준화는 이상치에 민감하고, 정규화는 이상치에 덜 민감하다."],
        "model_answer": "표준화는 평균 0, 표준편차 1로 조정하고, 정규화는 데이터를 0과 1 사이로 조정한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] NLP",
        "question_text": "어텐션(Attention) 메커니즘이 기존의 Seq2Seq 모델이 가진 '고정 길이 문맥 벡터(Bottleneck)' 문제를 해결하는 방식은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["입력 시퀀스의 모든 단어를 하나의 벡터로 압축한다.", "디코더가 각 타임스텝에서 출력 단어를 생성할 때, 입력 시퀀스의 특정 부분에 더 집중(가중치 부여)할 수 있도록 한다.", "입력 시퀀스의 길이를 강제로 고정시킨다.", "은닉층의 개수를 늘려 모델의 용량을 키운다."],
        "model_answer": "디코더가 각 타임스텝에서 출력 단어를 생성할 때, 입력 시퀀스의 특정 부분에 더 집중(가중치 부여)할 수 있도록 한다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "지시 학습(Instruction Tuning)의 주요 목표는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["LLM이 더 많은 사실적 지식을 암기하도록 하는 것", "LLM이 사람의 지시를 더 잘 따르고 의도에 맞는 응답을 생성하도록 하는 것", "LLM의 사전 학습 속도를 높이는 것", "LLM이 생성하는 텍스트의 길이를 늘리는 것"],
        "model_answer": "LLM이 사람의 지시를 더 잘 따르고 의도에 맞는 응답을 생성하도록 하는 것",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "모델 가지치기(Pruning) 기법 중, 모델의 특정 구조(채널, 필터 등) 단위로 가중치를 제거하는 방식은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["비정형 가지치기 (Unstructured Pruning)", "정형 가지치기 (Structured Pruning)", "크기 기반 가지치기 (Magnitude-based Pruning)", "랜덤 가지치기 (Random Pruning)"],
        "model_answer": "정형 가지치기 (Structured Pruning)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "지식 증류(Knowledge Distillation)의 핵심 아이디어는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["작은 모델 여러 개를 합쳐 큰 모델 하나를 만드는 것", "크고 성능이 좋은 교사 모델(Teacher Model)의 지식을 작고 빠른 학생 모델(Student Model)에게 전달하는 것", "데이터를 증강하여 모델의 일반화 성능을 높이는 것", "모델의 특정 레이어만 선택하여 학습시키는 것"],
        "model_answer": "크고 성능이 좋은 교사 모델(Teacher Model)의 지식을 작고 빠른 학생 모델(Student Model)에게 전달하는 것",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] NLP",
        "question_text": "텍스트 데이터의 전처리 단계에서 불용어(Stopwords)를 제거하는 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["문장의 길이를 늘려 모델의 입력 크기를 맞추기 위해", "문법적 오류를 수정하여 문장의 완성도를 높이기 위해", "'a', 'the', 'is'와 같이 의미 분석에 큰 도움이 되지 않으면서 빈번하게 등장하는 단어를 제거하여 분석 효율을 높이기 위해", "모든 단어를 소문자로 변환하여 일관성을 유지하기 위해"],
        "model_answer": "'a', 'the', 'is'와 같이 의미 분석에 큰 도움이 되지 않으면서 빈번하게 등장하는 단어를 제거하여 분석 효율을 높이기 위해",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Model Evaluation",
        "question_text": "분류 모델의 성능을 시각적으로 평가하는 데 사용되며, 실제 클래스와 모델이 예측한 클래스의 관계를 보여주는 행렬은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["상관 행렬 (Correlation Matrix)", "혼동 행렬 (Confusion Matrix)", "공분산 행렬 (Covariance Matrix)", "특징 중요도 행렬 (Feature Importance Matrix)"],
        "model_answer": "혼동 행렬 (Confusion Matrix)",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Machine Learning",
        "question_text": "K-평균 군집화(K-Means Clustering) 알고리즘에 대한 설명으로 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["데이터를 미리 정해진 K개의 군집으로 그룹화하는 비지도 학습 알고리즘이다.", "데이터를 회귀선을 기준으로 분류하는 지도 학습 알고리즘이다.", "각 데이터 포인트에 대해 상과 벌을 부여하여 최적의 정책을 학습하는 강화 학습 알고리즘이다.", "데이터의 차원을 축소하여 시각화하는 데 사용되는 알고리즘이다."],
        "model_answer": "데이터를 미리 정해진 K개의 군집으로 그룹화하는 비지도 학습 알고리즘이다.",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "딥러닝에서 '에포크(Epoch)'는 무엇을 의미합니까?",
        "question_type": "multiple_choice",
        "options": ["한 번의 가중치 업데이트에 사용되는 데이터 샘플의 수", "전체 훈련 데이터셋이 신경망을 한 번 통과하는 과정", "모델의 손실 함수 값", "신경망의 전체 레이어 수"],
        "model_answer": "전체 훈련 데이터셋이 신경망을 한 번 통과하는 과정",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM에서 'Chain-of-Thought (CoT)' 프롬프팅 기법의 핵심 아이디어는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["모델에게 최종 답변만 요구하여 응답 속도를 높이는 것", "모델이 정답에 도달하기까지의 중간 추론 과정을 단계별로 생각하고 설명하도록 유도하여 복잡한 문제 해결 능력을 향상시키는 것", "질문에 대한 답변을 한 단어로만 생성하도록 제한하는 것", "모델에게 여러 예시를 보여주어 문체나 형식을 모방하도록 하는 것"],
        "model_answer": "모델이 정답에 도달하기까지의 중간 추론 과정을 단계별로 생각하고 설명하도록 유도하여 복잡한 문제 해결 능력을 향상시키는 것",
        "keywords_full_credit": [],
        "keywords_partial_credit": []
    },
    {
        "subject": "[이론] 머신러닝",
        "question_text": "K-Means 군집화에서 초기 중심점(centroid) 위치에 따라 결과가 달라질 수 있는 문제를 완화하기 위한 방법으로 가장 적절한 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["K값을 1로 설정한다", "데이터를 정규화한다", "여러 번 다른 초기 중심으로 실행하고 최상의 결과를 선택한다", "군집의 개수를 데이터 포인트의 개수와 동일하게 설정한다"],
        "model_answer": "여러 번 다른 초기 중심으로 실행하고 최상의 결과를 선택한다"
    },
    {
        "subject": "[이론] 머신러닝",
        "question_text": "계층적 군집(Hierarchical Clustering)에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["미리 군집의 개수(K)를 정해야 한다.", "데이터 포인트 간의 거리를 기반으로 가장 가까운 것부터 순차적으로 묶어나간다.", "모든 군집이 동일한 크기를 갖도록 보장한다.", "연속적인 값을 예측하는 데 사용된다."],
        "model_answer": "데이터 포인트 간의 거리를 기반으로 가장 가까운 것부터 순차적으로 묶어나간다."
    },
    {
        "subject": "[이론] 최적화",
        "question_text": "경사 하강법에서 학습률이 너무 클 때 발생할 수 있는 가장 대표적인 문제는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["학습 속도가 매우 느려진다.", "최적점에 수렴하지 못하고 발산(overshooting)할 수 있다.", "지역 최적점(local minimum)에 빠지기 쉽다.", "모델의 과소적합을 유발한다."],
        "model_answer": "최적점에 수렴하지 못하고 발산(overshooting)할 수 있다."
    },
    {
        "subject": "[이론] EDA",
        "question_text": "데이터의 분포를 시각화할 때, 특정 구간에 데이터가 얼마나 집중되어 있는지 막대 그래프 형태로 보여주는 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["산점도(Scatter Plot)", "선 그래프(Line Plot)", "히스토그램(Histogram)", "파이 차트(Pie Chart)"],
        "model_answer": "히스토그램(Histogram)"
    },
    {
        "subject": "[이론] 모델 평가",
        "question_text": "분류 모델의 평가지표 중, (TP + TN) / (TP + TN + FP + FN) 수식으로 계산되는 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["정밀도(Precision)", "재현율(Recall)", "정확도(Accuracy)", "F1-Score"],
        "model_answer": "정확도(Accuracy)"
    },
    {
        "subject": "[이론] 딥러닝",
        "question_text": "딥러닝 모델의 학습 과정에서 손실(loss)이 더 이상 감소하지 않고 검증 데이터에 대한 성능이 저하되기 시작할 때 학습을 중단하는 기법은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["조기 종료(Early Stopping)", "경사 하강법(Gradient Descent)", "드롭아웃(Dropout)", "배치 정규화(Batch Normalization)"],
        "model_answer": "조기 종료(Early Stopping)"
    },
    {
        "subject": "[이론] 딥러닝",
        "question_text": "L1 규제(Lasso)가 L2 규제(Ridge)와 구별되는 가장 큰 특징은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["가중치를 0에 가깝게 만들지만 0으로 만들지는 않는다.", "일부 가중치를 완전히 0으로 만들어 특성 선택(feature selection) 효과를 가진다.", "모델의 복잡도에 영향을 주지 않는다.", "항상 L2 규제보다 더 나은 성능을 보인다."],
        "model_answer": "일부 가중치를 완전히 0으로 만들어 특성 선택(feature selection) 효과를 가진다."
    },
    {
        "subject": "[이론] 딥러닝",
        "question_text": "딥러닝에서, 신경망의 각 레이어를 통과한 데이터의 분포가 바뀌는 현상(Internal Covariate Shift)을 완화하여 학습을 안정시키는 기법은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["드롭아웃(Dropout)", "배치 정규화(Batch Normalization)", "가중치 초기화(Weight Initialization)", "활성화 함수(Activation Function)"],
        "model_answer": "배치 정규화(Batch Normalization)"
    },
    {
        "subject": "[이론] CV",
        "question_text": "CNN에서, 입력 데이터의 채널 수는 유지하면서 공간적 크기(가로, 세로)를 줄이는 데 사용되는 연산은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["1x1 합성곱", "패딩(Padding)", "풀링(Pooling)", "업샘플링(Upsampling)"],
        "model_answer": "풀링(Pooling)"
    },
    {
        "subject": "[이론] NLP",
        "question_text": "순차적 데이터의 맥락을 양방향으로 고려하기 위해 RNN을 변형한 모델은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["단방향 RNN (Unidirectional RNN)", "양방향 RNN (Bidirectional RNN)", "심층 RNN (Deep RNN)", "합성곱 RNN (Convolutional RNN)"],
        "model_answer": "양방향 RNN (Bidirectional RNN)"
    },
    {
        "subject": "[이론] NLP",
        "question_text": "LSTM의 셀 상태(Cell State)가 하는 역할로 가장 적절한 설명은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["현재 타임스텝의 입력만을 처리한다.", "시퀀스의 장기적인 정보를 컨베이어 벨트처럼 전달하여 장기 의존성 문제를 해결한다.", "모델의 최종 출력을 결정한다.", "인접한 단어 간의 관계만 학습한다."],
        "model_answer": "시퀀스의 장기적인 정보를 컨베이어 벨트처럼 전달하여 장기 의존성 문제를 해결한다."
    },
    {
        "subject": "[이론] NLP",
        "question_text": "트랜스포머의 멀티-헤드 어텐션(Multi-Head Attention)이 단일 어텐션에 비해 갖는 장점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["계산 속도가 더 빠르다.", "더 적은 메모리를 사용한다.", "다양한 관점에서 단어 간의 관계를 동시에 학습할 수 있다.", "항상 더 짧은 문장을 생성한다."],
        "model_answer": "다양한 관점에서 단어 간의 관계를 동시에 학습할 수 있다."
    },
    {
        "subject": "[이론] NLP",
        "question_text": "BERT 모델의 사전 학습에서, 두 문장이 연속된 문장인지 아닌지를 예측하도록 학습하는 태스크는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["MLM (Masked Language Model)", "NSP (Next Sentence Prediction)", "QA (Question Answering)", "NLI (Natural Language Inference)"],
        "model_answer": "NSP (Next Sentence Prediction)"
    },
    {
        "subject": "[이론] CV",
        "question_text": "CNN 아키텍처 중, 동일한 크기의 필터를 반복적으로 깊게 쌓는 단순한 구조를 통해 깊이의 중요성을 입증한 모델은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["AlexNet", "VGGNet", "GoogLeNet (Inception)", "ResNet"],
        "model_answer": "VGGNet"
    },
    {
        "subject": "[이론] CV",
        "question_text": "ViT(Vision Transformer)에서 입력 이미지를 패치로 나눈 후, 각 패치의 순서 정보를 모델에 제공하기 위해 더해지는 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["클래스 토큰 (Class Token)", "위치 임베딩 (Positional Embedding)", "세그먼트 ID (Segment ID)", "어텐션 스코어 (Attention Score)"],
        "model_answer": "위치 임베딩 (Positional Embedding)"
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG 시스템에서, 검색된 문서(context)와 사용자 질문을 함께 LLM에 입력하여 답변을 생성하게 하는 주된 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["모델의 응답 속도를 높이기 위해", "모델이 사실에 기반하고 더 정확한 답변을 생성하도록 유도하기 위해", "모델의 창의성을 극대화하기 위해", "입력 텍스트의 길이를 줄이기 위해"],
        "model_answer": "모델이 사실에 기반하고 더 정확한 답변을 생성하도록 유도하기 위해"
    },
    {
        "subject": "[이론] PEFT",
        "question_text": "PEFT 기법 중, 모델의 가중치 행렬에 저차원(low-rank) 행렬을 추가하여 학습 파라미터 수를 줄이는 방식은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Adapter Tuning", "Prefix Tuning", "Prompt Tuning", "LoRA"],
        "model_answer": "LoRA"
    },
    {
        "subject": "[이론] PEFT",
        "question_text": "LoRA에서, 학습된 어댑터 가중치의 영향력을 조절하는 스케일링 값으로 사용되는 하이퍼파라미터는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["r (rank)", "lora_alpha", "lora_dropout", "bias"],
        "model_answer": "lora_alpha"
    },
    {
        "subject": "[이론] PEFT",
        "question_text": "모델의 가중치나 활성화 값을 더 적은 비트(bit)로 표현하여 모델 크기를 줄이고 추론 속도를 향상시키는 기술은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["양자화(Quantization)", "가지치기(Pruning)", "지식 증류(Knowledge Distillation)", "LoRA"],
        "model_answer": "양자화(Quantization)"
    },
    {
        "subject": "[이론] Foundation Model",
        "question_text": "LLM의 규모가 커짐에 따라 이전에 없던 새로운 능력이 나타나는 현상을 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["규모의 법칙(Scaling Law)", "창발적 능력(Emergent Abilities)", "인컨텍스트 학습(In-Context Learning)", "환각(Hallucination)"],
        "model_answer": "창발적 능력(Emergent Abilities)"
    },
    {
        "subject": "[이론] NLP",
        "question_text": "Word2Vec의 Skip-gram 모델이 CBOW에 비해 갖는 장점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["학습 속도가 더 빠르다.", "더 큰 데이터셋에 적합하다.", "희귀 단어에 대한 임베딩 품질이 더 우수하다.", "메모리 사용량이 더 적다."],
        "model_answer": "희귀 단어에 대한 임베딩 품질이 더 우수하다."
    },
    {
        "subject": "[이론] Model Evaluation",
        "question_text": "데이터 불균형이 심한 분류 문제에서 다수 클래스를 모두 Negative로 예측해도 정확도(Accuracy)가 높게 나타나는 문제를 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["정확도의 역설(Accuracy Paradox)", "과적합(Overfitting)", "F1 스코어의 함정", "재현율의 한계"],
        "model_answer": "정확도의 역설(Accuracy Paradox)"
    },
    {
        "subject": "[이론] Deep Learning",
        "question_text": "과적합(Overfitting)을 완화하기 위한 데이터 증강(Data Augmentation) 기법이 아닌 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["이미지 회전 및 좌우 반전", "학습 데이터에 노이즈 추가", "훈련 데이터의 일부를 복제하여 양 늘리기", "생성 모델(GAN 등)을 사용하여 새로운 학습 데이터 생성"],
        "model_answer": "훈련 데이터의 일부를 복제하여 양 늘리기"
    },
    {
        "subject": "[이론] CV",
        "question_text": "CNN에서 스트라이드(Stride) 값을 1보다 크게 설정했을 때 얻는 효과는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["특징 맵의 해상도를 높인다.", "특징 맵의 공간적 차원을 줄인다(Downsampling).", "모델의 파라미터 수를 늘린다.", "기울기 소실 문제를 악화시킨다."],
        "model_answer": "특징 맵의 공간적 차원을 줄인다(Downsampling)."
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM의 생성적 답변의 다양성을 조절하기 위해 사용되며, 값이 높을수록 더 무작위적인 샘플링을 유도하는 파라미터는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Temperature", "Top-p", "Top-k", "Beam width"],
        "model_answer": "Temperature"
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM의 디코딩 전략 중, 여러 개의 후보 시퀀스(beam)를 유지하며 가장 가능성 있는 문장을 탐색하는 방식은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Greedy Decoding", "Beam Search", "Top-K Sampling", "Nucleus Sampling"],
        "model_answer": "Beam Search"
    },
    {
        "subject": "[이론] LLM",
        "question_text": "RLHF에서, 사람의 선호도 데이터를 학습하여 특정 응답이 얼마나 좋은지를 평가하는 모델은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["정책 모델(Policy Model)", "가치 모델(Value Model)", "보상 모델(Reward Model)", "환경 모델(Environment Model)"],
        "model_answer": "보상 모델(Reward Model)"
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG 시스템에서 검색의 정확도를 높이기 위해, Bi-encoder로 후보군을 추리고 Cross-encoder로 재순위(re-ranking)를 매기는 방식을 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["단일 단계 검색(Single-stage Retrieval)", "2단계 검색(Two-stage Retrieval)", "희소 검색(Sparse Retrieval)", "조밀 검색(Dense Retrieval)"],
        "model_answer": "2단계 검색(Two-stage Retrieval)"
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG에서 사용되는 Sparse Retriever와 Dense Retriever의 가장 큰 차이점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Sparse Retriever는 키워드 기반, Dense Retriever는 의미 기반으로 검색한다.", "Sparse Retriever는 벡터를 사용하고, Dense Retriever는 키워드를 사용한다.", "Sparse Retriever는 항상 더 빠르다.", "Dense Retriever는 색인 생성이 필요 없다."],
        "model_answer": "Sparse Retriever는 키워드 기반, Dense Retriever는 의미 기반으로 검색한다."
    },
    {
        "subject": "[이론] 머신러닝",
        "question_text": "PCA(주성분 분석)에 대한 설명으로 가장 올바른 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["데이터를 여러 군집으로 나누는 알고리즘이다.", "데이터의 분산을 최대한 보존하는 새로운 축(주성분)을 찾아 차원을 축소하는 기법이다.", "데이터의 클래스를 분류하는 지도 학습 알고리즘이다.", "결측치를 대체하는 데 사용되는 기법이다."],
        "model_answer": "데이터의 분산을 최대한 보존하는 새로운 축(주성분)을 찾아 차원을 축소하는 기법이다."
    },
    {
        "subject": "[이론] 데이터 전처리",
        "question_text": "데이터 스케일링 기법 중, 이상치(outlier)의 영향을 가장 많이 받는 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["표준화(Standardization)", "정규화(Min-Max Scaling)", "로버스트 스케일링(Robust Scaling)", "로그 변환(Log Transform)"],
        "model_answer": "정규화(Min-Max Scaling)"
    },
    {
        "subject": "[이론] NLP",
        "question_text": "트랜스포머 디코더에서, 현재 위치의 단어가 미래 위치의 단어 정보를 참고하지 못하도록 막는 장치는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["패딩 마스크(Padding Mask)", "룩-어헤드 마스크(Look-Ahead Mask)", "세그먼트 마스크(Segment Mask)", "어텐션 스코어(Attention Score)"],
        "model_answer": "룩-어헤드 마스크(Look-Ahead Mask)"
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM이 사람의 가치나 의도에 부합하는 답변을 생성하도록 유도하는 전체적인 과정을 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["사전 학습(Pre-training)", "정렬(Alignment)", "양자화(Quantization)", "추론(Inference)"],
        "model_answer": "정렬(Alignment)"
    },
    {
        "subject": "[이론] PEFT",
        "question_text": "모델의 중요도가 낮은 가중치를 제거하여 모델을 경량화하는 Pruning 기법과 Quantization의 차이점은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["Pruning은 가중치를 제거하고, Quantization은 가중치의 정밀도를 낮춘다.", "Pruning은 가중치의 정밀도를 낮추고, Quantization은 가중치를 제거한다.", "두 기법은 동일한 목적을 가진 다른 이름이다.", "Pruning은 학습 중에만, Quantization은 추론 중에만 적용된다."],
        "model_answer": "Pruning은 가중치를 제거하고, Quantization은 가중치의 정밀도를 낮춘다."
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM을 평가하는 벤치마크 중, 초등학교 수준부터 전문가 수준까지 다양한 주제의 객관식 문제를 통해 모델의 지식과 추론 능력을 측정하는 것은 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["GLUE", "SuperGLUE", "MMLU", "SQuAD"],
        "model_answer": "MMLU"
    },
    {
        "subject": "[이론] RAG",
        "question_text": "LangChain에서 LLM, 프롬프트 템플릿, 출력 파서 등을 파이프(|) 기호로 연결하여 데이터 흐름을 만드는 방식을 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["LCEL (LangChain Expression Language)", "Agent Executor", "Callback Handler", "Toolchain"],
        "model_answer": "LCEL (LangChain Expression Language)"
    },
    {
        "subject": "[이론] 모델 평가",
        "question_text": "혼동 행렬(Confusion Matrix)에서 실제는 Negative인데 Positive로 잘못 예측한 경우를 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["True Positive (TP)", "True Negative (TN)", "False Positive (FP)", "False Negative (FN)"],
        "model_answer": "False Positive (FP)"
    },
    {
        "subject": "[이론] 모델 학습",
        "question_text": "딥러닝 모델이 학습을 반복함에 따라 훈련 데이터에 대한 손실은 계속 감소하지만, 검증 데이터에 대한 손실이 증가하기 시작하는 지점은 무엇을 시사합니까?",
        "question_type": "multiple_choice",
        "options": ["모델이 과소적합되고 있다.", "모델이 과적합되기 시작했다.", "학습률이 너무 낮다.", "최적의 모델에 도달했다."],
        "model_answer": "모델이 과적합되기 시작했다."
    },
    {
        "subject": "[이론] LLM",
        "question_text": "LLM이 안전 정책을 우회하여 유해하거나 부적절한 콘텐츠를 생성하도록 유도하는 프롬프팅 기법을 무엇이라고 합니까?",
        "question_type": "multiple_choice",
        "options": ["Chain-of-Thought", "Zero-shot Prompting", "Jailbreaking", "Instruction Tuning"],
        "model_answer": "Jailbreaking"
    },
    {
        "subject": "[이론] RAG",
        "question_text": "RAG 시스템에서 하이브리드 검색(Hybrid Search)이 필요한 이유는 무엇입니까?",
        "question_type": "multiple_choice",
        "options": ["키워드 기반 검색(Sparse)과 의미 기반 검색(Dense)의 장점을 결합하여 검색 성능을 높이기 위해", "검색 속도를 최대한 느리게 만들기 위해", "오직 키워드가 정확히 일치하는 문서만 찾기 위해", "검색 결과의 다양성을 줄이기 위해"],
        "model_answer": "키워드 기반 검색(Sparse)과 의미 기반 검색(Dense)의 장점을 결합하여 검색 성능을 높이기 위해"
    }
]